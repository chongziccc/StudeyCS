> 机器学习：
>

- 监督学习
- 无监督学习



##### 监督学习：

- 回归：从正确的结果中学习，从无限可能中预测正确的结果，比如房价预测
- 分类：对类别进行一个预测，所有中的一小部分可能得结果

###### 无监督学习（聚类算法）：不给定标签，数据都与y无关，对特定数据集看看有没有什么模式或结构，可能分成不同的集合

新闻，就是把相关的信息聚集在一起。获取没有标签的数据自动将其归入集群中

##### 无监督学习：聚类，异常，降维



######  回归模型：

- 线性回归模型

构建回归模型，函数很重要，构建成本函数，帮助机器学习

f_w,b(x)=y中的，w,b是模型参数可以调整，简写成f(x)=y

设置好的w,b然后让函数能更好准确的表达Y值

!![image-20250719121735300](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250719121735300.png)

线性回归中常用的函数，适合回归问题

成本函数，用wb然后找到J的成本函数的值，从而画出成本函数，取最小值的wb来训练机器，给出合适的wb得到合适的拟合线

![image-20250719140410160](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250719140410160.png)



梯度下降算法：（在山顶环顾四周想去山谷，那个方向迈出一小步，然后一直到最终目的地）用于训练线性回归和AI中一些最大最复杂的模型，也用于训练一些先进的神经网络模型，也称为深度学习模型，

更适合一般模型，已经两个以上的参数，使用的时候可以默认值都给0，然后细微调整参数来获取最小值J

![image-20250719145117093](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250719145117093.png)



α是表示学习率也就是下上的步子大小，越大步子越大反之相同，后面的求导是告诉你往哪个方向走，左侧的wb是新值，右侧的是旧值



![image-20250719150705573](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250719150705573.png)

y/x的值就是斜率，也就是该点的导数



![image-20250719150813946](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250719150813946.png)

首先这里的α是正数，w-正数会越来越小，往左取值，符合我们求最小的J值

![image-20250719163835017](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250719163835017.png)

学习率太大了，导致数据的幅度大，想到最小值，就得调整幅度，不然可能发散无法收敛

梯度下降算法：α后面就是对w求了偏导

![image-20250719165316525](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250719165316525.png)





不同的wb的值得到不同的局部最小值：

![image-20250719165529019](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250719165529019.png)

![image-20250719165554440](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250719165554440.png)

以上说明了线性回归如何通过梯度下降来实现



降属性数据进行向量化，用向量点积来简化表示方式

![image-20250719172814080](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250719172814080.png)



![image-20250719172627557](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250719172627557.png)

输入多个特征的线性回归模型被称为多元线性回归



使用向量化来简化计算，用到了Python中的numpy库,这种方式代码简洁，运行速度快（dot函数会并行调用电脑硬件）

![image-20250719173402391](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250719173402391.png)

多元线性回归梯度下降算法，其中的W相当于权重

![image-20250719212433932](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250719212433932.png)



特征缩放技术，是梯度算法下降速度更快，一个特征的可能值取得大的时候，参数的合理值会相对小，反之也成立，这个基本没什么缺点，然后缩放值不合理可以继续缩放

![image-20250720124218384](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720124218384.png)

当特征值的差距较大的时候，直接使用梯度下降算法，会来回弹不容易找到最小的值，使用特征缩放使在相似范围内取值，加速梯度下降

![image-20250720124541455](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720124541455.png)





均值归一化μ1是x1的均值，以此递推，均值归一化 -1到1之间，有点浮动没问题

![image-20250720125535439](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720125535439.png)



z-score归一化

![image-20250720224400696](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720224400696.png)



![image-20250720224516174](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720224516174.png)









![image-20250720125926399](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720125926399.png)





![image-20250720130104531](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720130104531.png)



检查梯度是否有效的下降，通过收敛，更合适的α

左图的横坐标是迭代次数，右图是找一个合适的极小值，然后J小于这个值就是收敛了，一般不采用自动的，找一个合适的小值比较困难

![image-20250720140930232](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720140930232.png)



合适的学习率

![image-20250720141451773](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720141451773.png)

图中的话α是学习率，如果玲w1=w1+ad1 这里的d1是导数，会导致原理小值反而上升所有得减，学习率有效的时候j值会在迭代的过程中变小



特征选择对算法也是十分重要的，创建一个新的特征就是特征工程的一个例子，例如房子的长宽各作为一个因素，长乘宽作为一个新的特征可以让拟合的线不仅仅是直线可以是曲线，非线性函数





多项式回归

![image-20250720142824262](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720142824262.png)



![image-20250720143139029](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720143139029.png)

线性回归解决很多问题不合适，我们考虑到逻辑回归

下图是直线拟合，如果说加一个例子的时候分0-1就从0.5变成0.7的了，影响正确结果

![image-20250720150047758](C:\Users\98501\Desktop\image-20250720150047758.png)



下图是曲线拟合，右侧的是逻辑函数（）取值在0-1，

![image-20250720150849681](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720150849681.png)

> 逻辑回归（Logistic Regression）是一个常见的 **分类算法**，尽管名字中包含“回归”，但它实际上是用来做 **分类** 的。其核心思想是 **通过一个回归模型** 来预测某个样本属于某个类别的概率。它广泛应用于二分类问题，如垃圾邮件识别、疾病预测等。
>
> ### **简单理解逻辑回归**
>
> 1. **基本概念**：
>
>    - **回归模型**：逻辑回归本质上是一种回归模型，通过线性组合输入特征，得到一个 **数值**。
>    - **概率输出**：不同于线性回归直接输出一个数值，逻辑回归使用 **sigmoid 函数**（又叫逻辑函数）将这个数值映射到 0 到 1 之间，用来表示 **某个样本属于某一类别的概率**。
>
>    例如，假设我们有一个学生的学习时间 `x`，我们想预测学生考试及格的概率，逻辑回归就是通过回归模型和 sigmoid 函数，输出一个 0 到 1 之间的数值，代表“学生及格”的概率。
>
> 2. **公式**：
>     逻辑回归的公式如下：
>    $$
>    y^=11+e−z=11+e−(w0+w1⋅x1+w2⋅x2+...+wn⋅xn)\hat{y} = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-(w_0 + w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_n \cdot x_n)}}
>    $$
>    其中：
>
>    - $$
>      z=w0+w1⋅x1+w2⋅x2+...+wn⋅xnz = w_0 + w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_n \cdot x_n
>      $$
>
>      是一个线性回归模型的输出。
>
>    - **sigmoid 函数**：
>      $$
>      y^\hat{y}
>      $$
>      是通过 **sigmoid 函数**将线性输出 zz 映射到 [0, 1] 区间，表示属于某个类别的概率。
>
>    这样，逻辑回归的输出就是 **样本属于某一类别的概率**。
>
> 3. **Sigmoid 函数**：
>
>    - Sigmoid 函数是一个 S 形的函数，其公式为：
>
>    $$
>    σ(z)=11+e−z\sigma(z) = \frac{1}{1 + e^{-z}}
>    $$
>
>    它将输入的任意实数 zz 转换为一个 0 到 1 之间的值。当 zz 较大时，输出接近 1，表示该样本属于“正类”的概率较高；当 zz 较小时，输出接近 0，表示属于“负类”的概率较高。
>
> 4. **决策阈值**：
>
>    - 逻辑回归的输出是一个概率值 
>      $$
>      y^\hat{y}
>      $$
>      ，我们需要设置一个 **决策阈值** 来决定将样本划分为哪一类。例如：
>
>      - 如果
>        $$
>        y^≥0.5\hat{y} \geq 0.5
>        $$
>        ，则预测样本属于正类（例如，及格）。
>
>      - 如果 
>        $$
>        y^<0.5\hat{y} < 0.5
>        $$
>        ，则预测样本属于负类（例如，不及格）。
>
>    这个 **阈值** 可以根据实际情况进行调整，比如在需要更高精度的应用中，可能需要将阈值设置为更高的值。
>
> 5. **训练过程**：
>
>    - 训练逻辑回归模型时，我们需要根据 **训练数据** 来调整参数（即 w0,w1,w2,…,wnw_0, w_1, w_2, \dots, w_n）。这是通过 **最小化损失函数**（常用的是 **交叉熵损失函数**）来实现的。
>    - 目标是通过优化算法（如梯度下降）最小化模型的预测误差，使得模型能够准确预测新样本的类别。
>
> ### **逻辑回归的优缺点**
>
> #### **优点**：
>
> 1. **简单易懂**：逻辑回归的数学公式和原理简单，容易理解，适合用于初学者。
> 2. **效率高**：逻辑回归计算量较小，尤其是在特征维度较低时，训练速度较快。
> 3. **概率输出**：能够输出属于某一类别的概率，可以用来做不确定性量化。
>
> #### **缺点**：
>
> 1. **仅适合线性问题**：逻辑回归本质上是一个线性模型，对于 **复杂的非线性问题**，表现可能较差。
> 2. **容易过拟合**：如果特征太多或者特征和标签之间关系过于复杂，逻辑回归容易出现过拟合现象。
>
> ### **总结**
>
> - 逻辑回归是一种常见的 **分类模型**，其目标是通过输入特征来预测样本属于某一类别的概率。
> - 通过 **线性回归** 和 **sigmoid 函数**，将预测值映射到 0 到 1 之间，用于做分类决策。
> - 它简单且高效，但在处理复杂的非线性数据时可能表现较差。
>
> 逻辑回归的核心思想是：通过一个线性组合来计算出某个事件发生的概率，再通过阈值来决定分类。



下图就是逻辑回归模型，作用是输入一个或者一组特征，然后得到一个0到1之间的数值，逻辑回归的输入就是输出y=1概率，可以这样理解；结果是0or1就是二元分类

![image-20250720151958093](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720151958093.png)



决策边界就是这条线两侧的结果是不同的，决策边界z=0

![image-20250720162158147](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720162158147.png)



![image-20250720162411961](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720162411961.png)

![image-20250720162703289](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720162703289.png)





逻辑回归的成本函数

![image-20250720163116321](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720163116321.png)

L逻辑损失函数，预测值接近1是损失小，接近0损失大，避免损失大，是一个凸函数，是整个训练集的函数

![image-20250720163238015](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720163238015.png)

预测的y的值和y的真实的值的损失

![image-20250720163737213](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720163737213.png)

![image-20250720163638761](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720163638761.png)

![image-20250720170254421](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720170254421.png)

![image-20250720170408735](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720170408735.png)

![image-20250720170556481](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720170556481.png)

![image-20250720172524042](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720172524042.png)

![image-20250720172702588](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720172702588.png)

很好的预测数据，称为算法的泛化，就是一个不在上面的数据也能良好预测

图三的圈圈，当大小对应的数据不准确的时候，误差大，线性算法的二过拟合三欠拟合

![image-20250720173748983](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720173748983.png)

| 方面         | 过拟合（Overfitting）                                        | 欠拟合（Underfitting）                         |
| ------------ | ------------------------------------------------------------ | ---------------------------------------------- |
| **定义**     | 模型过于复杂，能够记住训练数据中的噪声和细节，无法泛化到新数据。 | 模型过于简单，无法从训练数据中提取有效的模式。 |
| **训练误差** | 训练误差较低，模型在训练数据上表现良好。                     | 训练误差较高，模型在训练数据上也表现差。       |
| **测试误差** | 测试误差较高，模型无法泛化到新数据。                         | 测试误差较高，模型不能有效捕捉数据规律。       |
| **原因**     | 模型复杂度过高，训练数据不足或不够多样。                     | 模型复杂度过低，特征选择不合理或训练不足。     |
| **解决方法** | 正则化、增加训练数据、减少模型复杂度。                       | 增加模型复杂度、使用更多特征、延长训练时间。   |

![image-20250720174532019](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720174532019.png)

对于过度拟合，就是增加训练集数据，或者是减少特征数据（该模式容易丢失数据），或者正则化



拟合一个过高次数的多项式的话，得到过度拟合，通过对W的控制，避免成为二次函数，高次也有影响，但是不是过度拟合了

![image-20250720180033003](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720180033003.png)



**线性回归的成本函数**（带有正则化项）n是特征量个数

![image-20250720180059414](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720180059414.png)

后面的λ那一块就是正则表示，λ和α一样也是选择数值λ，拉姆达>0取值



> 
>
> 从这张图来看，展示的是 **逻辑回归模型** 或 **线性回归模型** 的 **成本函数**（Cost Function），它也包含了 **正则化项**（Regularization Term）。
>
> ### **解释公式**
>
> 公式的形式为：
>
> $$
> J(\mathbf{w}, b) = \frac{1}{2m} \sum_{i=1}^{m} \left( f_{\mathbf{w}, b} (\mathbf{x}^{(i)}) - y^{(i)} \right)^2 + \frac{\lambda}{2m} \sum_{j=1}^{n} w_j^2
> $$
>
> #### 1. **第一个部分：误差项（损失函数）**
>
> $$
> \frac{1}{2m} \sum_{i=1}^{m} \left( f_{\mathbf{w}, b} (\mathbf{x}^{(i)}) - y^{(i)} \right)^2
> $$
>
> * 这里 
>   $$
>   f_{\mathbf{w}, b} (\mathbf{x}^{(i)})
>   $$
>   是模型的预测值，表示用模型权重 $
>   $$
>   \mathbf{w}
>   $$
>   $ 和偏置 $b$ 对第 $i$ 个样本 
>   $$
>   $\mathbf{x}^{(i)}$
>   $$
>   的预测结果。
>   
>   * 对于线性回归，模型输出为 $
>     $$
>     \hat{y}^{(i)} = \mathbf{w}^T \mathbf{x}^{(i)} + b
>     $$
>     $。
> * $
>   $$
>   y^{(i)}
>   $$
>   $ 是第 $i$ 个样本的真实标签。
> * 这个部分是 **均方误差（MSE）**，用于度量模型预测值与真实值之间的差距。
> * **平均误差**：将每个样本的误差（预测值和真实值的差）平方后求和，然后除以样本数量 $m$（训练集中的样本总数）。
> * 这个项的目的是 **最小化误差**，使模型尽可能准确地预测样本。
>
> #### 2. **第二个部分：正则化项（L2正则化）**
>
> $$
> \frac{\lambda}{2m} \sum_{j=1}^{n} w_j^2
> $$
>
> * 这个部分是 **L2 正则化项**，用于防止模型过拟合（即，模型在训练集上表现很好，但无法推广到新数据）。
> * $$
>   $\lambda$
>   $$
>
>   是 **正则化超参数**，控制正则化的强度。较大的 
>   $$
>   $\lambda$
>   $$
>    值会加强对模型复杂度的惩罚，导致模型参数变得更小；而较小的 $\lambda$ 值则会减少对模型复杂度的限制。
> * $w_j$ 是模型的 **权重参数**，这个正则化项会惩罚过大的权重值。
> * 通过将所有权重的平方和加到成本函数中，**正则化项** 使得模型尽量避免过拟合，并保持模型的简单性。
>
> ### **整体理解**
>
> * **损失函数**（第一个部分）用于衡量模型预测的误差，即模型在训练集上的表现。
> * **正则化项**（第二个部分）用于控制模型的复杂度，避免模型过度拟合训练数据，保证它在未见过的数据上也有良好的表现。
>
> **完整的成本函数** 就是这两部分的加权和，通过调整 $\lambda$ 和优化权重 $\mathbf{w}$ 和偏置 $b$，最小化该函数，达到最好的模型拟合。
>
> ### **正则化的作用**
>
> * **L2 正则化**（即权重的平方和）鼓励模型使用更小的权重值，因此它有助于减少过拟合。通过限制模型的复杂度，防止模型对训练数据中的噪声进行过多的学习。
>
> ### **总结**
>
> * **第一个部分** 是 **均方误差（MSE）**，用于衡量模型的预测与实际标签之间的差异。
> * **第二个部分** 是 **正则化项**，通过惩罚模型的权重，控制模型复杂度，防止过拟合。
> * **整体目标**：通过 **梯度下降** 等优化算法最小化 **成本函数**，即在保证误差最小的同时，避免模型过于复杂。

![image-20250720182008780](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250720182008780.png)

不正则化b所以不动b



![image-20250721113514743](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250721113514743.png)





正则化逻辑回归

![image-20250721114201763](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250721114201763.png)

多个神经网络被称为多层感知器

![image-20250721133532065](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250721133532065.png)

构建神经元，每一个圈圈是一个特征值，然后神经元算出值，根据规则产生结果然后给下一层的神经元

![image-20250721140315595](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250721140315595.png)

![image-20250721140812562](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250721140812562.png)

一层到一层，到最后的输出结果，四层的神经网络的话，输入层不参与计数

![image-20250721142242242](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250721142242242.png)

上图中的a向量是激活值，由上一层的激活值和这一层的w向量计算得到下一层的激活值











在一层层的激活，也就是向前传播

![image-20250721143218579](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250721143218579.png)







TensorFlow是深度学习算法领先的一个框架另一个是pytorch



![image-20250721150517100](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250721150517100.png)

![image-20250722101040899](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250722101040899.png)

神经网络可以发展因为可以矢量化，然后采用矩阵乘法高效的实现，gpu很擅长矩阵乘法

神经网络中向前传播的矢量代码

![image-20250722104137566](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250722104137566.png)





AT就是转置也等于A.T  ，，，`np.matmul` 是 NumPy 中用于进行 **矩阵乘法** 的函数 z=AT@w是另一一种调用matmul的方法



TensorFlow中实现

![image-20250722110535812](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250722110535812.png)

![image-20250722111129166](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250722111129166.png)





![image-20250722112158809](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250722112158809.png)

在一个fit的函数中实现这些反向传播







除了z的值激活函数还有别的，线性激活函数g=g(z),g(z)=max(0,z),因为z=z所以有时候说没有使用激活函数，其实不对的

relu函数是的神经网络学习更快

![image-20250722112922834](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250722112922834.png)

![image-20250722122103181](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250722122103181.png)

![image-20250723161429503](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250723161429503.png)

softmax逻辑回归公式 



![image-20250723163211099](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250723163211099.png)

a1+a2+.....an会等于1



softmax的遗失函数

![image-20250723191814890](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250723191814890.png)



多标签分类问题定义

![image-20250723194601567](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250723194601567.png)







Adam可以自动调节学习率

![image-20250723195206355](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250723195206355.png)



多个卷积层就称为卷积神经网络

卷积每次都是查看一个区间

![image-20250723201910773](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250723201910773.png)



训练的时候可以大部分数据来训练，小部分来测试7:3



交叉验证，60%训练的，20%交叉验证，20%测试

![image-20250723205506804](C:\Users\98501\AppData\Roaming\Typora\typora-user-images\image-20250723205506804.png)
